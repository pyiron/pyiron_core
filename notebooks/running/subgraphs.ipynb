{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.160114Z",
     "start_time": "2025-04-30T20:40:42.801245Z"
    }
   },
   "source": "import pyiron_core as pc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.286536Z",
     "start_time": "2025-04-30T20:40:43.162307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pc.as_inp_dataclass_node\n",
    "class MyInpData:\n",
    "    question: str = \"still computing\"\n",
    "    answer: int = 42\n",
    "\n",
    "@pc.as_out_dataclass_node\n",
    "class MyOutData:\n",
    "    question: str = \"still computing\"\n",
    "    answer: int = 42\n",
    "\n",
    "wf = pc.Workflow(\"auto_encoding_graph\")\n",
    "wf.to_dataclass = MyInpData(\"input will be output\", 0)\n",
    "wf.from_dataclass = MyOutData(wf.to_dataclass)\n",
    "wf.run()"
   ],
   "id": "24b31153cb427a01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input will be output', 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.412819Z",
     "start_time": "2025-04-30T20:40:43.408409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pc.as_macro_node([\"q\", \"a\"])\n",
    "def auto_encoder(q: str, a: int):\n",
    "    wf = pc.Workflow(\"auto_encoding_subgraph\")\n",
    "    wf.to_dataclass = MyInpData(q, a)\n",
    "    wf.from_dataclass = MyOutData(wf.to_dataclass)\n",
    "    return wf.from_dataclass.outputs.question, wf.from_dataclass.outputs.answer\n",
    "\n",
    "macro = auto_encoder()\n",
    "macro(\"also when it's a macro\", 1)"
   ],
   "id": "af6bc96ff44ee655",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"also when it's a macro\", 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's hidden behind a private variable, but we can investigate the internal state of the subgraph -- i.e. the macro object contains its subgraph's retrospective provenance",
   "id": "fef1858f8337d92f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.421425Z",
     "start_time": "2025-04-30T20:40:43.419718Z"
    }
   },
   "cell_type": "code",
   "source": "macro._wf_macro.to_dataclass.outputs.dataclass.value",
   "id": "bcb835c61f03052f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyInpData(question=\"also when it's a macro\", answer=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`Workflow` objects are already inspectable, since we can always look at their children:",
   "id": "9b796e8f70a1ee0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.434905Z",
     "start_time": "2025-04-30T20:40:43.428970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wf = pc.Workflow(\"my_workflow\")\n",
    "wf.to_dataclass = MyInpData(\"provenance is great\", 2)\n",
    "wf.from_dataclass = MyOutData(wf.to_dataclass.outputs.dataclass)\n",
    "wf.macro = auto_encoder(wf.from_dataclass.outputs.question, wf.from_dataclass.outputs.answer)\n",
    "wf.run()"
   ],
   "id": "3f4014e1cdace84b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('provenance is great', 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.447559Z",
     "start_time": "2025-04-30T20:40:43.445614Z"
    }
   },
   "cell_type": "code",
   "source": "wf.macro._wf_macro.to_dataclass.outputs.dataclass.value",
   "id": "a2aa637b307c7cc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyInpData(question='provenance is great', answer=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And we can nest macros, maintaining provenance to arbitrary depth",
   "id": "3a54c99787ec2efa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.464307Z",
     "start_time": "2025-04-30T20:40:43.457353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pc.as_macro_node([\"deep_q\", \"deep_a\"])\n",
    "def nested_macro(q: str, a: int):\n",
    "    wf = pc.Workflow(\"nested_macro_subgraph\")\n",
    "    wf.to_dataclass = MyInpData(q, a)\n",
    "    wf.from_dataclass = MyOutData(wf.to_dataclass)\n",
    "    wf.sub_macro = auto_encoder(wf.from_dataclass.outputs.question, wf.from_dataclass.outputs.answer)\n",
    "    return wf.sub_macro.outputs.q, wf.sub_macro.outputs.a\n",
    "\n",
    "nm = nested_macro()\n",
    "nm(\"we might want to make this optional so the garbage collector can save us memory\", 3)"
   ],
   "id": "cb9af61dc6f0c44a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('we might want to make this optional so the garbage collector can save us memory',\n",
       " 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.476215Z",
     "start_time": "2025-04-30T20:40:43.474331Z"
    }
   },
   "cell_type": "code",
   "source": "nm._wf_macro.sub_macro._wf_macro.to_dataclass.outputs.dataclass.value",
   "id": "33b9a3e05e3d7e84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyInpData(question='we might want to make this optional so the garbage collector can save us memory', answer=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T20:40:43.487081Z",
     "start_time": "2025-04-30T20:40:43.485874Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c54df0babc8aae25",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
